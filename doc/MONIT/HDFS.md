# Spark and SWAN

This is a collection of tutorials and methods to access data on HDFS. 
First of all you need to get authorised to access a Spark-based cluster at CERN (analytix is the general purpose cluster for spark-based workflows)
- HDFS from lxplus7: use method described [here](https://cern.service-now.com/service-portal/article.do?n=KB0004426)
- How to connect to a CERN Spark cluster from [lxplus7](https://cern.service-now.com/service-portal/article.do?n=KB0004426) or [SWAN](https://github.com/swan-cern/help/blob/master/spark/clusters.md) 


## Tutorials from CERN IT
- [Guide](http://hadoop-user-guide.web.cern.ch/hadoop-user-guide/) to CERN Hadoop services
- [HDFS access at CERN](https://hadoop-user-guide.web.cern.ch)
- [Setup](https://hadoop-user-guide.web.cern.ch/hadoop-user-guide/getstart/client_conf_with_puppet_.html) your own VM to act as Hadoop client
- [Interacting](https://hadoop-user-guide.web.cern.ch/hadoop-user-guide/getstart/client_edge_machine.html) with Hadoop cluster via edge nodes
- Documentation about the [SWAN](https://github.com/swan-cern/help) service

## CMS Tutorials

- [CMSSpark framework](https://github.com/vkuznet/CMSSpark/wiki)
- [Spark code](https://github.com/nsmith-/cms-working-set) to read data on HDFS for popularity study
- [CMSSpark SWAN notebook](https://github.com/dmwm/CMSSpark/blob/master/src/notebooks/CMSSparkExample.ipynb) on data popularity
- [Swan notebook](https://cernbox.cern.ch/index.php/s/gBClptQGPs80CpP) to read in job data (/project/awg/cms/job-monitoring/avro-snappy/)

## Rumble

- [Documentation](https://github.com/dmwm/CMSMonitoring/blob/master/doc/Rumble/README.md)
